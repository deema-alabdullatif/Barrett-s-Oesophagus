function [cost, grad, preds] = cnnCost(theta,images,labels,numClasses,...
    filterDim,numFilters,poolDim,pred)
% Calcualte cost and gradient for a single layer convolutional
% neural network followed by a softmax layer with cross entropy
% objective.
%
% Parameters:
%  theta      -  unrolled parameter vector
%  images     -  stores images in imageDim x imageDim x numImges
%                array
%  numClasses -  number of classes to predict
%  filterDim  -  dimension of convolutional filter
%  numFilters -  number of convolutional filters
%  poolDim    -  dimension of pooling area
%  pred       -  boolean only forward propagate and return
%                predictions
%
%
% Returns:
%  cost       -  cross entropy cost
%  grad       -  gradient with respect to theta (if pred==False)
%  preds      -  list of predictions for each example (if pred==True)

filters1=6;
filters2=12;

if ~exist('pred','var')
    pred = false;
end;


imageDim = size(images,1); % height/width of image
numImages = size(images,3); % number of images

%% Reshape parameters and setup gradient matrices

% Wc is filterDim x filterDim x numFilters parameter matrix
% bc is the corresponding bias

% Wd is numClasses x hiddenSize parameter matrix where hiddenSize
% is the number of output units from the convolutional layer
% bd is corresponding bias
[Wc, Wd, bc, bd] = cnnParamsToStack(theta,imageDim,filterDim,numFilters,...
    poolDim,numClasses,filters1,filters2);

% Same sizes as Wc,Wd,bc,bd. Used to hold gradient w.r.t above params.
Wc_grad = zeros(size(Wc));
Wd_grad = zeros(size(Wd));
bc_grad = zeros(size(bc));
bd_grad = zeros(size(bd));
% Wc_S = size(Wc)
% Wd_S = size(Wd)
% bc_S = size(bc)
% bd_s=size(bd)
%%======================================================================
%% STEP 1a: Forward Propagation
%  In this step you will forward propagate the input through the
%  convolutional and subsampling (mean pooling) layers.  You will then use
%  the responses from the convolution and pooling layer as the input to a
%  standard softmax layer.

%% Convolutional Layer
%  For each image and each filter, convolve the image with the filter, add
%  the bias and apply the sigmoid nonlinearity.  Then subsample the
%  convolved activations with mean pooling.  Store the results of the
%  convolution in activations and the results of the pooling in
%  activationsPooled.  You will need to save the convolved activations for
%  backpropagation.
convDim = imageDim-filterDim+1; % dimension of convolved output
outputDim = (convDim)/poolDim; % dimension of subsampled output

% convDim x convDim x numFilters x numImages tensor for storing activations
activations1 = zeros(convDim,convDim,filters1,numImages);%numFilters=6------------------------->

% outputDim x outputDim x numFilters x numImages tensor for storing
% subsampled activations
activationsPooled1 = zeros(outputDim,outputDim,filters1,numImages);%numFilters=6------------------------->
% size(activationsPooled1)
%%% YOUR CODE HERE %%%
activations1 = cnnConvolve(filterDim, filters1, images, Wc(:,:,1:filters1), bc(1:filters1),1,filters1,filters2,'s');%numFilters=6------------------------->
% a=size(activations1)
activationsPooled1 = cnnPool_1(poolDim, activations1);

% % % % Reshape activations into 2-d matrix, hiddenSize x numImages,
% % % % for Softmax layer
% % % % activationsPooled = reshape(activationsPooled1,[],numImages);
% % % % PRE_before=size(activationsPooled)
%my addition---------------------------------------------------------------------------------------------->

activations = zeros(((size(activationsPooled1,1))-filterDim+1),((size(activationsPooled1,2))-filterDim+1),(filters2*filters1),numImages);
% before=size(activations)
activationsPooled = zeros((size(activations,1)/poolDim),(size(activations,2)/poolDim),(filters2*filters1),numImages);
% size(activationsPooled)
activations = cnnConvolve(filterDim,filters2, activationsPooled1, Wc(:,:,filters1+1:filters1+filters2), bc(filters1+1:filters1+filters2),2,filters1,filters2,'s');%numFilters=6------------------------->
% after=size(activations)
activationsPooled = cnnPool_1(poolDim, activations);
% pool_before=size(activationsPooled)

% Reshape activations into 2-d matrix, hiddenSize x numImages,
% for Softmax layer
% numImages
activationsPooled = reshape(activationsPooled,[],numImages);
% pool_before=size(activationsPooled)
%-------------------------------------------------------------------------end addition
%% Softmax Layer
%  Forward propagate the pooled activations calculated above into a
%  standard softmax layer. For your convenience we have reshaped
%  activationPooled into a hiddenSize x numImages matrix.  Store the
%  results in probs.

% numClasses x numImages for storing probability that each image belongs to
% each class.
probs = zeros(numClasses,numImages);
M = size(images, 3);
%%% YOUR CODE HERE %%%

aux1 = Wd*activationsPooled + repmat(bd,1,M);%1:6------------------------->
var2 = bsxfun(@minus, aux1, max(aux1, [], 1)); %Substracts the maximm value of the matrix "aux1".
var3 = exp(var2);
probs = bsxfun(@rdivide, var3, sum(var3)); %It divides the vector "var3" by the sum of its elements.
% clear aux1;
clear var2;
clear var3;
%%======================================================================
%% STEP 1b: Calculate Cost
%  In this step you will use the labels given as input and the probs
%  calculate above to evaluate the cross entropy objective.  Store your
%  results in cost.
groundTruth = full(sparse(labels, 1:M, 1));
% % % load groundTruth;
var4 = groundTruth.*probs;
variable = log(var4(var4 ~= 0)); %Extract non-zero entries.
%== Obly term of cost function: fidelity term
cost = -mean(variable);
clear var4;
clear variable;
%%% YOUR CODE HERE %%%

% Makes predictions given probs and returns without backproagating errors.
if pred
    [~,preds] = max(probs,[],1);
    preds = preds';
    grad = 0;
    %     preds = probs
    return;
end;

%%======================================================================
%% STEP 1c: Backpropagation for the second layer
%  Backpropagate errors through the softmax and convolutional/subsampling
%  layers.  Store the errors for the next step to calculate the gradient.
%  Backpropagating the error w.r.t the softmax layer is as usual.  To
%  backpropagate through the pooling layer, you will need to upsample the
%  error with respect to the pooling layer for each filter and each image.
%  Use the kron function and a matrix of ones to do this upsampling
%  quickly.

%%% YOUR CODE HERE %%%
%1 calculate the error from Softmax
deriv_1 = (-1/M).*(groundTruth - probs);
clear groundTruth;


%2 calculate the error from mean pooling layer
deriv_2_pooled_sh = Wd'*deriv_1;
% size(Wd)
% size(deriv_1)
% size(deriv_2_pooled_sh)
outputDim=(outputDim-filterDim+1)/poolDim;

deriv_2_pooled = reshape(deriv_2_pooled_sh,outputDim,outputDim,filters1*filters2,numImages);
deriv_2_upsampled = zeros(outputDim*poolDim,outputDim*poolDim,filters1*filters2,numImages);


% deema2=size(deriv_2_pooled)



for imageNum = 1:numImages
    
    
    for filterNum = 1:filters1*filters2
        %==   Upsampling:
        %   Upsample the incoming error using kron
        %I dont need the following line as I will use the max pool i need to use
        %the max pools ones
        var3 = (1/(poolDim^2)).*kron(squeeze(deriv_2_pooled(:,:,filterNum,imageNum)),ones(poolDim));
        
        %             three=size(var3)
        
        %3 calculate the error in convolutional layer
        %we used sigmoid activation type this is why we use sigmoid derivative=
        %x.*(1-x)
        deriv_2_upsampled(:,:,filterNum,imageNum) = var3.*activations(:,:,filterNum,imageNum).*(1-activations(:,:,filterNum,imageNum));
        %
        %             size(deriv_2_upsampled)
        
    end
end

%%======================================================================
%% STEP 1d: Gradient Calculation
%  After backpropagating the errors above, we can use them to calculate the
%  gradient with respect to all the parameters.  The gradient w.r.t the
%  softmax layer is calculated as usual.  To calculate the gradient w.r.t.
%  a filter in the convolutional layer, convolve the backpropagated error
%  for that filter with each image and aggregate over images.

%%% YOUR CODE HERE %%%
%a) calculate gradient for the Softmax layer
Wd_grad = deriv_1*activationsPooled';
% clear activationsPooled;
bd_grad = deriv_1*ones(M,1);

clear deriv_1;



for imageNum = 1:numImages
    ind=1;
    
    for filterNum = 1:filters2
        for patch=1:size(activationsPooled1,3)
            % % %                 i=imageNum
            % % %                 f=filterNum+filters1
            % % %                 p=patch
            % % %                 in=ind
            im = squeeze(activationsPooled1(:,:,patch,imageNum));
            %==   Convolution:
            %             i=size(im)
            %     im = squeeze(images(:,:,imageNum));
            f_now = squeeze(deriv_2_upsampled(:,:,ind,imageNum));
            %             now=size(f_now)
            noww = conv2(im,rot90(squeeze(f_now),2),'valid');
            %             no=    size(noww)
            %     size(Wc_grad(:,:,filterNum))
            Wc_grad(:,:,filterNum+filters1) = squeeze(Wc_grad(:,:,filterNum+filters1)) + noww;
            bc_grad(filterNum+filters1) = bc_grad(filterNum+filters1) + sum(f_now(:));
            %     activations = zeros(convDim,convDim,numFilters,numImages);
            
            % %   Convolution of the current image with average kernel:
            %     current_image = squeeze(convolvedFeatures(:,:,filterNum,imageNum));
            %     Img_conv = conv2(current_image,avg_kern,'valid');
            % %   Upsampling to get the correct
            %     aux = downsample(Img_conv,poolDim);
            %     aux1 = downsample(aux',poolDim);
            %     aux1 = aux1';
            % %
            %     pooledFeatures(:,:,filterNum,imageNum) = aux1;
            ind=ind+1;
        end
    end
end


%%======================================================================
%% STEP 1c: Backpropagation for the first layer
%  Backpropagate errors through convolutional/subsampling
%  layers.  Store the errors for the next step to calculate the gradient.
%  Backpropagating the error w.r.t the softmax layer is as usual.  To
%  backpropagate through the pooling layer, you will need to upsample the
%  error with respect to the pooling layer for each filter and each image.
%  Use the kron function and a matrix of ones to do this upsampling
%  quickly.

%%% YOUR CODE HERE %%%
% % % % % %

% % % % % %
% % % % % % %2 calculate the error from mean pooling layer
% % % % % % deriv_2_pooled_sh = Wd'*deriv_1;
% % % % % % outputDim=(outputDim-filterDim+1)/poolDim;

convDim = imageDim-filterDim+1; % dimension of convolved output
outputDim = (convDim)/poolDim; % dimension of subsampled output
z=zeros(outputDim,outputDim,filters1,numImages);
for n=1:numImages
    for i=1:filters1
        ind=1;
        for j=1:filters2
            z(:,:,i,n)=z(:,:,i,n)+convn(deriv_2_upsampled(:,:,ind,n), rot180(Wc(:,:,j+6)), 'full');
            ind=ind+1;
        end
    end
end

% deriv_2_pooled = reshape(z,outputDim,outputDim,filters1*filters2,numImages);



deriv_2_upsamplede2 = zeros(convDim,convDim,filters1,numImages);




for imageNum = 1:numImages
    im = squeeze(images(:,:,imageNum));
    for filterNum = 1:filters1
        %==   Upsampling:
        %     var2 = upsample(deriv_2_pooled(:,:,filterNum,imageNum),poolDim);
        %     var3 = upsample(var2',poolDim);
        %     deriv_2_upsampled(:,:,filterNum,imageNum) = (var3').*activations(:,:,filterNum,imageNum).*(1-activations(:,:,filterNum,imageNum));
        %   Upsample the incoming error using kron
        var4 = (1/(poolDim^2)).*kron(squeeze(z(:,:,filterNum,imageNum)),ones(poolDim));
        %         size(var4)
        deriv_2_upsampled2(:,:,filterNum,imageNum) = var4.*activations1(:,:,filterNum,imageNum).*(1-activations1(:,:,filterNum,imageNum));
        %==   Convolution:
        %     im = squeeze(images(:,:,imageNum));
        f_now = squeeze(deriv_2_upsampled2(:,:,filterNum,imageNum));
        noww = conv2(im,rot90(squeeze(f_now),2),'valid');
        %     size(noww)
        %     size(Wc_grad(:,:,filterNum))
        Wc_grad(:,:,filterNum) = squeeze(Wc_grad(:,:,filterNum)) + noww;
        bc_grad(filterNum) = bc_grad(filterNum) + sum(f_now(:));
        %     activations = zeros(convDim,convDim,numFilters,numImages);
        
        % %   Convolution of the current image with average kernel:
        %     current_image = squeeze(convolvedFeatures(:,:,filterNum,imageNum));
        %     Img_conv = conv2(current_image,avg_kern,'valid');
        % %   Upsampling to get the correct
        %     aux = downsample(Img_conv,poolDim);
        %     aux1 = downsample(aux',poolDim);
        %     aux1 = aux1';
        % %
        %     pooledFeatures(:,:,filterNum,imageNum) = aux1;
    end
end


global w;
w=Wc_grad;

%%======================================================================


clear activations;
clear deriv_2_pooled;
clear deriv_2_upsampled;


%% Unroll gradient into grad vector for minFunc
grad = [Wc_grad(:) ; Wd_grad(:) ; bc_grad(:) ; bd_grad(:)];
    function X = rot180(X)
        X = flipdim(flipdim(X, 1), 2);
    end
end
