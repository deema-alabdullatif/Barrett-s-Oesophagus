%% Convolution Neural Network Exercise

%  Instructions
%  ------------
% 
%  This file contains code that helps you get started in building a single.
%  layer convolutional nerual network. In this exercise, you will only
%  need to modify cnnCost.m and cnnminFuncSGD.m. You will not need to 
%  modify this file.

%%======================================================================
%% STEP 0: Initialize Parameters and Load Data
%  Here we initialize some parameters used for the exercise.

% Configuration
imageDim = 500;
% % % % % % % % numClasses = 10;  % Number of classes (MNIST images fall into 10 classes)
numClasses = 3;
filterDim = 15;    % Filter size for conv layer
numFilters = 18;   % Number of filters for conv layer
poolDim = 6;      % Pooling dimension, (should divide imageDim-filterDim+1)

% Load MNIST Train
% addpath ../common/;
% images = loadMNISTImages('train-images.idx3-ubyte');
% images = reshape(images,imageDim,imageDim,[]);
% labels = loadMNISTLabels('train-labels.idx1-ubyte');


% % load hchannel;
% images=fourtytrain;
% % load images;
% % load labels;
% labels=fourtytrainL;
% load hchannel;
% images=himages;
% % % for i=1:403
% % % level=graythresh(uint8(himages(:,:,i)));
% % % bw=im2bw(uint8(himages(:,:,i)),level);
% % % images(:,:,i)=imcomplement(bw);
% % % % c=imcomplement(bw);
% % % % images(:,:,i)=himages(:,:,i).*c;
% % % end
% load hchanneltest;


% load labels;






% % Here we load MNIST training images
% addpath ../common/;
% images = loadMNISTImages('../common/train-images-idx3-ubyte');

images = reshape(images,imageDim,imageDim,[]);
% labels = loadMNISTLabels('../common/train-labels-idx1-ubyte');

% 
% 
% labels(labels==0) = 10; % Remap 0 to 10

% Initialize Parameters
theta = cnnInitParams(imageDim,filterDim,numFilters,poolDim,numClasses,6,12);

%%======================================================================
%% STEP 1: Implement convNet Objective
%  Implement the function cnnCost.m.

%%======================================================================
%% STEP 2: Gradient Check
%  Use the file computeNumericalGradient.m to check the gradient
%  calculation for your cnnCost.m function.  You may need to add the
%  appropriate path or copy the file to this directory.

DEBUG=false;  % set this to true to check gradient
if DEBUG
    % To speed up gradient checking, we will use a reduced network and
    % a debugging data set
    db_numFilters = 2;
    db_filterDim = 9;
    db_poolDim = 5;
    db_images = images(:,:,1:10);
    db_labels = labels(1:10);
    db_theta = cnnInitParams(imageDim,db_filterDim,db_numFilters,...
                db_poolDim,numClasses);
    
    [cost grad] = cnnCost(db_theta,db_images,db_labels,numClasses,...
                                db_filterDim,db_numFilters,db_poolDim);
    

    % Check gradients
    numGrad = computeNumericalGradient( @(x) cnnCost(x,db_images,...
                                db_labels,numClasses,db_filterDim,...
                                db_numFilters,db_poolDim), db_theta);
 
    % Use this to visually compare the gradients side by side
    disp([numGrad grad]);
    
    diff = norm(numGrad-grad)/norm(numGrad+grad);
    % Should be small. In our implementation, these values are usually 
    % less than 1e-9.
    disp(diff); 
 
    assert(diff < 1e-9,...
        'Difference too large. Check your gradient computation again');
    
end;

%%======================================================================
%% STEP 3: Learn Parameters
%  Implement minFuncSGD.m, then train the model.

options.epochs =1;
options.minibatch = 100;
options.alpha = 1e-1;
options.momentum = .95;
% 
opttheta = minFuncSGD(@(x,y,z) cnnCost(x,y,z,numClasses,filterDim,...
                      numFilters,poolDim),theta,images,labels,options);
% save('cnn_parameters.mat', 'opttheta');

global w;
global a;
a;
w;
%%======================================================================
%% STEP 4: Test
%  Test the performance of the trained model using the MNIST test set. Your
%  accuracy should be above 97% after 3 epochs of training
load hchanneltest;
testImages=himages2;
% % for i=1:30
% % level=graythresh(uint8(himages2(:,:,i)));
% % bw=im2bw(uint8(himages2(:,:,i)),level);
% % testImages(:,:,i)=imcomplement(bw);
% % % c=imcomplement(bw);
% % % testImages(:,:,i)=himages2(:,:,i).*c;
% % end
load testLabels;

% load hchanneltest;
% testImages=fourtytest;
% load testImages;
% load testLabels;
% testLabels=fourtytestL;
% testImages = loadMNISTImages('../common/t10k-images-idx3-ubyte');
% testImages = reshape(testImages,imageDim,imageDim,[]);
% testLabels = loadMNISTLabels('../common/t10k-labels-idx1-ubyte');
% testImages = loadMNISTImages('t10k-images.idx3-ubyte');
% testImages = reshape(testImages,imageDim,imageDim,[]);
% testLabels = loadMNISTLabels('t10k-labels.idx1-ubyte');
% testLabels(testLabels==0) = 10; % Remap 0 to 10

[~,cost,preds]=cnnCost(opttheta,testImages,testLabels,numClasses, filterDim,numFilters,poolDim,true);

acc = (sum(preds==testLabels)/length(preds))*100;

% Accuracy should be around 97.4% after 3 epochs
fprintf('Accuracy is %f percent\n',acc);
